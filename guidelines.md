

# Automated Textbook Explainer Video Generator â€“ **Revised with Hybrid Semantic Retrieval**

## 150â€‘Word Executive Summary

Automated Textbook Explainer Video Generator is a oneâ€‘day hackathon prototype that converts any OpenStax PDF chapter into a concise, studentâ€‘friendly video explanation. A minimalist Streamlit UI lets students select a chapter/section or type a freeâ€‘text question. Under the hood: the PDF is parsed into hierarchical chunks, vectorâ€‘indexed with embeddings, and searched via a hybrid retriever that returns only the three most relevant leaf chunks. GPTâ€‘4o turns those chunks into a 60â€‘second instructional script, which Dâ€‘ID renders as a talkingâ€‘head video. The entire flow runs locally with Python, Streamlit, ChromaDB, and two external APIs (OpenAI, Dâ€‘ID). The result is a demoâ€‘ready tool that showcases fast document retrieval, LLMâ€‘powered pedagogy, and automated video generationâ€”all built in under a day.

---

# **Project Roadmap (Revised)**

## Section 0 Â· Preâ€‘Hackathon Admin  *20 min*  âœ… **COMPLETED**

1. Collect `OPENAI_API_KEY`, `DID_API_KEY`.
2. Download/crop sample PDF.
3. Install Python 3.11, create venv.
4. Open project folder in Cursor, stash keys locally.

## Section 1 Â· Repository & Baseline  *15 min*  âœ… **COMPLETED**

Initialize git, `.gitignore`, `README.md`, MIT license, first commit.

## Section 2 Â· Project Skeleton  *25 min*  âœ… **COMPLETED**

**Current Structure:**
```
openstax-vid-gen/
â”œâ”€â”€ app.py                                    # Streamlit UI placeholder
â”œâ”€â”€ video_maker.py                            # Orchestration placeholder
â”œâ”€â”€ indexer.py                                # Embedding & ChromaDB indexing
â”œâ”€â”€ retriever.py                              # Hybrid search with MMR
â”œâ”€â”€ test_chromadb.py                          # Testing utility
â”‚
â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ pdf_parser.py                         # Hierarchical PDF chunking
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ explainer.py                          # LLM script generator placeholder
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ video/
â”‚   â”œâ”€â”€ did_client.py                         # D-ID API client placeholder
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ chroma/                               # ChromaDB vector database
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ requirements.txt                          # Updated dependencies
â”œâ”€â”€ sample_physics_cropped.pdf                # Cropped textbook
â”œâ”€â”€ sample_physics_cropped_leaf_chunks.json   # Generated chunks
â”œâ”€â”€ .env.example
â””â”€â”€ â€¦
```

## Section 3 Â· Environment & Deps  *15 min*  âœ… **COMPLETED**

**Updated `requirements.txt`:**
```
streamlit
pymupdf
langchain
chromadb
tiktoken
openai
requests
python-dotenv
```

## Section 4 Â· PDF Parsing & Chunking  *50 min*  âœ… **COMPLETED**

**Implementation:** `parsers/pdf_parser.py`
* âœ… Detects section headings using regex patterns (`CHAPTER X`, `X.Y Title`)
* âœ… Splits each section into ~350-token leaf chunks with 20-token overlap using tiktoken
* âœ… Returns list of dicts with metadata: `chapter`, `section`, `title`, `chunk_index`, `text`, `start_page`, `end_page`
* âœ… Outputs JSON file: `sample_physics_cropped_leaf_chunks.json` (218 chunks)
* âœ… **Test:** `python parsers/pdf_parser.py` â†’ generates chunked JSON

## Section 5 Â· Embedding & Vector Index  *25 min*  âœ… **COMPLETED**

**Implementation:** `indexer.py`
* âœ… Embeds every leaf chunk using OpenAI's `text-embedding-3-small`
* âœ… Stores 1536-dimensional vectors + metadata in local ChromaDB (`cache/chroma/`)
* âœ… Batch processing (32 chunks/batch) with progress bar
* âœ… **Test:** `python indexer.py` â†’ embeds 218 chunks, stores in ChromaDB
* âœ… **Verification:** `python test_chromadb.py` â†’ inspects database and tests queries

## Section 6 Â· Hybrid Retriever  *45 min*  âœ… **COMPLETED**

**Implementation:** `retriever.py`
* âœ… **Simplified architecture** (no LangChain dependencies due to deprecation issues)
* âœ… **Metadata parsing:** Detects section/chapter queries using regex
* âœ… **Hybrid search:** Combines metadata filtering + cosine similarity
* âœ… **MMR diversity:** Returns top 3 diverse, relevant chunks
* âœ… **Robust fallbacks:** Falls back to similarity search if metadata filtering fails
* âœ… **Test:** `python retriever.py` â†’ tests 5 different query types
* âœ… **Performance:** Correctly handles both content queries ("What are significant figures?") and structural queries ("What is section 1.1 about?")

## Section 7 Â· LLM Script Generator  *30 min*  **PENDING**

* `llm/explainer.py` â€“ GPT-4o integration to generate scripts from retrieved chunks
* Function signature: `generate_script(chunks: List[Dict]) -> str`
* Prompt engineering for 60-second educational videos

## Section 8 Â· Dâ€‘ID Video API  *30 min*  **PENDING**

* `video/did_client.py` â€“ D-ID API integration
* Function signature: `create_video(script: str) -> str` (returns video URL)
* Caching by script hash to avoid duplicate API calls

## Section 9 Â· Orchestration Pipeline  *20 min*  **PENDING**

**Updated `video_maker.py`:**
```python
def generate_video(query: str) -> str:
    # 1. Use retriever to get relevant chunks
    chunks = retriever.search(query, top_k=3)
    # 2. Generate script from chunks
    script = explainer.generate_script(chunks)
    # 3. Create video from script
    video_url = did_client.create_video(script)
    return video_url
```

## Section 10 Â· Streamlit UI  *45 min*  **PENDING**

* Update `app.py` with full UI
* Free-text input box for queries
* Optional chapter/section dropdown
* Video player for results
* Show retrieved chunks/pages for transparency

## Section 11 Â· Quality of Life  *30 min*  **PENDING**

Logging, token/cost tracker, preâ€‘commit, enhanced caching.

## Section 12 Â· Demo Prep & Submission  *45 min*  **PENDING**

Complete README, demo video/GIF, tag release.

---

# **Implementation Status & Changes Made**

## âœ… **Completed Sections (0-6)**

| Section | Status | Key Achievements |
|---------|--------|------------------|
| 0-3 | âœ… Complete | Environment setup, dependencies, project structure |
| 4 | âœ… Complete | **218 chunks** from cropped PDF, hierarchical chunking working |
| 5 | âœ… Complete | **ChromaDB** with 1536-dim embeddings, verified working |
| 6 | âœ… Complete | **Hybrid retriever** with metadata filtering + MMR, tested |

## ðŸ”§ **Key Implementation Changes**

| Original Plan | What Was Actually Built | Reason |
|---------------|------------------------|---------|
| LangChain SelfQueryRetriever | Custom metadata parsing + direct ChromaDB | LangChain deprecation warnings, missing dependencies |
| Complex BM25 + cosine hybrid | Metadata filtering + cosine similarity + MMR | Simpler, more reliable implementation |
| LangChain dependencies | Direct ChromaDB + OpenAI APIs | Fewer dependencies, more stable |

## ðŸ“Š **Current Performance Metrics**

* **PDF Processing:** 218 chunks from cropped textbook
* **Embedding Cost:** ~$0.01 for 218 chunks using text-embedding-3-small
* **Retrieval Speed:** <0.5s for query + MMR ranking
* **Accuracy:** Correctly retrieves relevant sections for both content and structural queries

---

## **Next Steps (Sections 7-12)**

The foundation is solid and tested. Ready to implement:
1. **LLM Script Generator** - GPT-4o integration
2. **D-ID Video API** - Video generation
3. **Orchestration** - Connect all components
4. **UI** - Streamlit interface
5. **Polish** - Logging, caching, demo prep

---

### Updated Implementation Notes

* **Embedding cost:** 218 leaf chunks â‰ˆ 65k tokens â†’ ~$0.01 with text-embedding-3-small
* **Runtime latency:** Retrieval < 0.5s; chunk quality verified through testing
* **Architecture:** Simplified but robust - direct API usage instead of complex frameworks
* **Testing:** All components have working test functions for verification

